# Install necessary libraries
!pip install transformers datasets tensorflow scikit-learn

# Step 1: Mount Google Drive
from google.colab import drive
drive.mount('/content/drive')

# Import libraries
import tensorflow as tf
from transformers import AutoTokenizer, TFAutoModelForSequenceClassification
from datasets import Dataset
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd
import re
from sklearn.preprocessing import LabelEncoder  # Add LabelEncoder

# Load and clean the dataset
dataset_path = '/content/drive/MyDrive/Colab Notebooks/thsab.csv'

# Function to automatically fix malformed rows
def clean_csv(file_path, delimiter='\t', expected_columns=2):
    valid_lines = []
    with open(file_path, 'r', encoding='utf-8') as file:
        for line in file:
            if line.count(delimiter) == expected_columns - 1:  # Check the number of columns
                valid_lines.append(line)

    # Save a new cleaned file
    cleaned_file_path = '/content/drive/MyDrive/Colab Notebooks/cleaned_thsab.csv'
    with open(cleaned_file_path, 'w', encoding='utf-8') as cleaned_file:
        cleaned_file.writelines(valid_lines)
    return cleaned_file_path

# Clean the CSV file
cleaned_dataset_path = clean_csv(dataset_path)

# Load the dataset with Pandas
data = pd.read_csv(cleaned_dataset_path, encoding='utf-8', delimiter='\t')  # Ensure correct delimiter

# Display column names to verify
print(data.columns)

# Rename columns if necessary
# Replace 'texte' and 'categorie' with actual column names in your file
data.rename(columns={'texte': 'text', 'categorie': 'label'}, inplace=True)  # If needed

# Check for required columns
if 'text' in data.columns and 'label' in data.columns:
    X = data['text'].astype(str)  # Convert to string
else:
    raise KeyError("Columns 'text' and 'label' were not found in the CSV file.")

# Convert labels to integers
label_encoder = LabelEncoder()
data['label'] = label_encoder.fit_transform(data['label'])  # Encode labels into integers (0, 1, 2)

y = data['label'].astype(int)  # Ensure labels are integers

# Step 4: Clean and normalize Arabic text
def clean_arabic_text(text):
    # Remove digits
    text = re.sub(r'\d+', '', text)

    # Remove punctuation
    text = re.sub(r'[^\w\s]', '', text)

    # Normalize Arabic characters
    text = text.replace('إ', 'ا').replace('أ', 'ا').replace('آ', 'ا').replace('ئ', 'ي')
    text = text.replace('ؤ', 'و').replace('ى', 'ي').replace('ة', 'ه')

    # Reduce repeated Arabic letters
    text = re.sub(r'(ا)+', 'ا', text)  # Replace multiple "ا" with one
    text = re.sub(r'(و)+', 'و', text)  # Replace multiple "و" with one
    text = re.sub(r'(ي)+', 'ي', text)  # Replace multiple "ي" with one

    # Remove extra spaces
    text = re.sub(r'\s+', ' ', text).strip()

    return text

# Apply the function to the 'text' column
data['text'] = data['text'].apply(clean_arabic_text)

# Split the data into train and test sets (80% train, 20% test)
X_train, X_test, y_train, y_test = train_test_split(data['text'], data['label'], test_size=0.2, random_state=42)

# Load the model and tokenizer
model_name = "AhmedBou/TuniBert"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = TFAutoModelForSequenceClassification.from_pretrained(model_name, from_pt=True)  # Load from PyTorch weights

# Tokenization
def preprocess_data(texts):
    return tokenizer(list(texts), truncation=True, padding=True, max_length=128, return_tensors="tf")

train_encodings = preprocess_data(X_train)
test_encodings = preprocess_data(X_test)

# Prepare TensorFlow datasets
train_dataset = tf.data.Dataset.from_tensor_slices((dict(train_encodings), y_train.values))
test_dataset = tf.data.Dataset.from_tensor_slices((dict(test_encodings), y_test.values))

batch_size = 16
train_dataset = train_dataset.shuffle(1000).batch(batch_size).prefetch(tf.data.AUTOTUNE)
test_dataset = test_dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE)

# Compile and train the model
optimizer = tf.keras.optimizers.Adam(learning_rate=5e-5)
loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)
metric = tf.keras.metrics.SparseCategoricalAccuracy()

model.compile(optimizer=optimizer, loss=loss, metrics=[metric])

history = model.fit(train_dataset, validation_data=test_dataset, epochs=3)

# Evaluation
y_pred = tf.nn.softmax(model.predict(test_dataset).logits).numpy()
y_pred_classes = y_pred.argmax(axis=1)

# Print classification results
print(classification_report(y_test, y_pred_classes, target_names=[str(cls) for cls in label_encoder.classes_]))

# Confusion matrix
conf_matrix = confusion_matrix(y_test, y_pred_classes)
plt.figure(figsize=(8, 6))
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=label_encoder.classes_, yticklabels=label_encoder.classes_)
plt.xlabel('Predictions')
plt.ylabel('True Labels')
plt.title('Confusion Matrix')
plt.show()
